{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]/root/man-kala/mancala_rl_agent.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  index = self.policy_net(torch.tensor(state, device=device, dtype=torch.float)).max(0).indices.view(1, 1).long()\n",
      "  0%|          | 0/6000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# print((f\"count: {t}\"))\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# print((\"selecting action...\"))\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m---> 40\u001b[0m     action_item \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# TODO: skip action/update/optimise if there's no valid action available\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print((f\"environment applying action: {action_item}...\"))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# print((f\"{env}\"))\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "import game\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mancala_rl_agent import RlMancalaPlayer, TAU\n",
    "from game import GameSimulator\n",
    "\n",
    "CHECKPOINT_EPISODE = 200\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 6000\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "agent = RlMancalaPlayer(\n",
    "    None,\n",
    ")\n",
    "\n",
    "p1 = game.RandomPlayer(game.RANDOM_SEED)\n",
    "game.FIRST_PLAYER_AGENT = True\n",
    "env: GameSimulator = game.GameSimulator([agent, p1])\n",
    "agent.env = env\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "\n",
    "    # Play opponent turn if agent is not the first player\n",
    "    if (game.FIRST_PLAYER_AGENT and env.turn == 1) or (\n",
    "            not game.FIRST_PLAYER_AGENT and env.turn == 0\n",
    "        ):\n",
    "        env.play_next_turn()\n",
    "        state = env.observation\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "\n",
    "    for t in count():\n",
    "        # print((f\"count: {t}\"))\n",
    "        # print((\"selecting action...\"))\n",
    "        action = agent.select_action(state)\n",
    "        action_item = action.item()\n",
    "        # TODO: skip action/update/optimise if there's no valid action available\n",
    "        # print((f\"environment applying action: {action_item}...\"))\n",
    "        # print((f\"{env}\"))\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # simulate opponent turn\n",
    "        if not done:\n",
    "            env.play_next_turn()\n",
    "            observation = env.observation\n",
    "\n",
    "        if terminated:\n",
    "            # TODO: when the game is over, we should update next state to be something else\n",
    "            next_state = torch.tensor(observation, dtype=torch.float, device=device)\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float, device=device)\n",
    "\n",
    "        # print((\"storing in memory...\"))\n",
    "        # Store the transition in memory\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # print((\"optimizing model....\"))\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        agent.optimize_model()\n",
    "\n",
    "        # print((\"update weights...\"))\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = agent.target_net.state_dict()\n",
    "        policy_net_state_dict = agent.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        agent.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        # print((f\"am done? {done}\"))\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "    if (i_episode % CHECKPOINT_EPISODE) == 0:\n",
    "        agent.save_model()\n",
    "        print((f'Completed {i_episode}'))\n",
    "        plot_durations(show_result=True)\n",
    "        # plt.ioff()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
